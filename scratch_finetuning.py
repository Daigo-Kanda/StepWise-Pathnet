import argparse
import datetime
import os

import tensorflow as tf
from tensorflow import keras
from tensorflow.python.keras.callbacks import ModelCheckpoint

import ITrackerData_person_tensor as data_gen


def main(args):
    # 画像サイズの設定
    image_shape = (args.image_size, args.image_size, 3)

    if args.finetuning:
        model = keras.models.load_model(args.trained_model)
        name = 'finetuning'
    else:
        # load model with random initial weights
        model = tf.keras.models.clone_model(keras.models.load_model(args.trained_model))
        # compile
        model.compile(optimizer='adam', loss='mse', metrics=['mae'])
        name = 'scratch'

    # generator setting
    data = data_gen.getData(batch_size=args.batch_size, memory_size=args.memory_size, dataset_path=args.dataset_dir)
    train_generator = data[0]
    validation_generator = data[1]
    test_generator = data[2]

    if not os.path.exists(args.save_dir):
        os.makedirs(args.save_dir)

    now = datetime.datetime.now()
    # callbacks
    cbks = [keras.callbacks.CSVLogger(
        os.path.join(args.save_dir, '%s_%s_%s.csv' % (name, "eye_tracking", now.strftime('%Y%m%d_%H%M%S')))),
        ModelCheckpoint(
            os.path.join(args.save_dir, "models.{}.hdf5".format(now.strftime('%Y%m%d_%H%M%S'))),
            monitor='val_loss', save_best_only=True,
            save_weights_only=False)
    ]

    # Fit the model on the batches generated by datagen.flow().
    history = model.fit(
        x=train_generator,
        initial_epoch=0,
        epochs=args.epochs,
        verbose=1,
        validation_data=validation_generator,
        callbacks=cbks,
    )

    # model.save(os.path.join(args.save_dir, 'model_finetuning_{}.hdf5'.format(now.strftime('%Y%m%d_%H%M%S'))))

    ## save history
    # df_history = pd.DataFrame(history.history)
    # df_history.to_csv(os.path.join(args.save_dir, 'finetuning_%s_%s.csv' % (args.model_name, args.learning_number)))

    tf.keras.backend.clear_session()


def getParser():
    parser = argparse.ArgumentParser()

    parser.add_argument('--memory_size', type=int, default=150)

    # scratch or finetuning
    parser.add_argument('--finetuning', action='store_true')

    # データセットのディレクトリ
    parser.add_argument('dataset_dir')

    # ログの保存先ディレクトリ
    parser.add_argument('save_dir')

    # クラス数
    parser.add_argument('--num_classes', type=int, default=100)

    # 学習データの画像数
    #   CIFAR10: 50000, CIFAR100: 50000
    parser.add_argument('--num_images_train', type=int, default=50000, help='CIFAR10: 50000, CIFAR100: 50000')

    # テストデータの画像数
    #   CIFAR10: 10000, CIFAR100: 10000
    parser.add_argument('--num_images_test', type=int, default=10000, help='CIFAR10: 10000, CIFAR100: 10000')

    # 画像サイズ
    parser.add_argument('--image_size', type=int, default=224)

    # バッチサイズ
    parser.add_argument('--batch_size', type=int, default=16)

    # エポック
    parser.add_argument('--epochs', type=int, default=60)

    # GPU並列
    parser.add_argument('--n_gpu', type=int, default=1)

    # CPU並列
    parser.add_argument('--n_thread', type=int, default=1)

    # CPU(スレッド)並列
    #   fit_generatorでスレッド並列するとデッドロックする臭い？
    #   https://github.com/keras-team/keras/issues/10340
    parser.add_argument('--use_multiprocessing', action='store_true')

    # 水増しの有無
    parser.add_argument('--dont_augment', action='store_false')

    # （テスト用）学習済みモデルの設定
    parser.add_argument('--trained_model', default=None)

    # 使う学習済みモデル
    parser.add_argument('--model_name', default='vgg16',
                        help='name of pre-trained network. this is disable by giving --trained_model')

    # 識別層の扱い
    parser.add_argument('--top_layer', default='replace', help='method for classification layer. (add, replace)')

    # 重み固定の有無
    parser.add_argument('--fix_pretrained', action='store_true')

    return parser


# 引数の読み込み
if __name__ == '__main__':
    parser = getParser()
    args = parser.parse_args()
    print('----PARSED ARGS----\n%s\n-----------------' % args)

    main(args)
