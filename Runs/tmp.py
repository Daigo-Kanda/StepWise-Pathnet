import glob
import glob
import os

import numpy as np
import tensorflow as tf

import ITrackerData_person_tensor as data_gen
import ITrackerData_person_tensor as ds

gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    # Restrict TensorFlow to only allocate 1GB of memory on the first GPU
    try:
        tf.config.experimental.set_memory_growth(gpus[0], True)

        logical_gpus = tf.config.experimental.list_logical_devices('GPU')
        print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPUs")
    except RuntimeError as e:
        # Virtual devices must be set before GPUs have been initialized
        print(e)

# hyper parameter
dataset_path = "/mnt/data/DataSet/GazeCapture_pre"
# server
# dataset_path = '/kanda_tmp/GazeCapture_pre'
model_path = "model/models.046-2.46558.hdf5"
participants_num = 30
loop_num = 5
batch_size = "64"
image_size = "224"
memory_size = '150'

participants_path = glob.glob(os.path.join(dataset_path, "**"))

participants_count = []
k = 0
for i, participant_path in enumerate(participants_path):
    metaFile = os.path.join(participant_path, 'metadata_person.mat')

    if os.path.exists(metaFile):
        participants_count.append(len(ds.loadMetadata(metaFile)['frameIndex']))
    else:
        participants_count.append(0)

tmp = zip(participants_count, participants_path)

# sorting
sorted_tmp = sorted(tmp, reverse=True)
participants_count, participants_path = zip(*sorted_tmp)

print(np.mean(participants_count))

data_path = '/mnt/data/DataSet/GazeCapture_pre/00804'
model_path = "/home/daigokanda/PycharmProjects/StepWise-Pathnet/stepwise_original/00804/models.20210108_201210.hdf5"

original_model = tf.keras.models.load_model(model_path)
output = original_model(original_model.inputs, training=False)
model = tf.keras.Model(original_model.inputs, output)
model.trainable = True
# compile
model.compile(optimizer=tf.keras.optimizers.Adam(1e-5), loss='mse', metrics=['mae'])

# generator setting
data = data_gen.getData(batch_size=256, memory_size=150, dataset_path=data_path)
train_generator = data[0]
validation_generator = data[1]
test_generator = data[2]

now = datetime.datetime.now()

# Fit the model on the batches generated by datagen.flow().
history = model.fit(
    x=train_generator,
    initial_epoch=0,
    epochs=30,
    verbose=1,
    validation_data=validation_generator,
)

#
# parser = tt.get_parser()
# history = tt.main(parser.parse_args(
#     [data_path, "./", "--image_size",
#      image_size, "--batch_size", batch_size,
#      "--epochs", "100", "--trained_model", model_path])
# )
#
# print(history)
